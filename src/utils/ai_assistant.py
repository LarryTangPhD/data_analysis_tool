"""
AIåŠ©æ‰‹æ¨¡å—
ä¸ºæ•°æ®åˆ†æåº”ç”¨æä¾›æ™ºèƒ½å»ºè®®ã€è§£é‡Šå’Œè¾…åŠ©åŠŸèƒ½
é‡æ„ç‰ˆæœ¬ï¼šå°†AIåŠŸèƒ½åˆ†æ•£åˆ°å„ä¸ªé¡µé¢ä¸­
"""

from langchain.prompts import ChatPromptTemplate
from langchain_openai.chat_models.base import BaseChatOpenAI
import os
import pandas as pd
import numpy as np
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings('ignore')


class DataAnalysisAI:
    """æ•°æ®åˆ†æAIåŠ©æ‰‹ç±»"""
    
    def __init__(self, api_key: Optional[str] = None, base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1"):
        """
        åˆå§‹åŒ–AIåŠ©æ‰‹
        
        Args:
            api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è·å–
            base_url: APIåŸºç¡€URL
        """
        self.api_key = api_key or os.getenv("DASHSCOPE_API_KEY")
        if not self.api_key:
            raise ValueError("DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®æˆ–æœªæä¾›")
        
        self.base_url = base_url
        self.llm = self._get_llm()
    
    def _get_llm(self, temperature: float = 0.7) -> BaseChatOpenAI:
        """
        åˆ›å»ºLLMå®ä¾‹
        
        Args:
            temperature: åˆ›é€ æ€§å‚æ•°
            
        Returns:
            BaseChatOpenAIå®ä¾‹
        """
        return BaseChatOpenAI(
            model="qwen-plus",
            openai_api_key=self.api_key,
            openai_api_base=self.base_url,
            temperature=temperature,
            max_tokens=2000,
            timeout=60,
            request_timeout=60
        )
    
    # ==================== æ•°æ®ä¸Šä¼ é¡µé¢AIåŠŸèƒ½ ====================
    
    def analyze_uploaded_data(self, data: pd.DataFrame, data_info: Dict[str, Any]) -> str:
        """
        åˆ†æä¸Šä¼ çš„æ•°æ®å¹¶æä¾›åˆå§‹å»ºè®®ï¼ˆæ•°æ®ä¸Šä¼ é¡µé¢ï¼‰
        
        Args:
            data: æ•°æ®æ¡†
            data_info: æ•°æ®åŸºæœ¬ä¿¡æ¯
            
        Returns:
            str: åˆ†æå»ºè®®
        """
        # è®¡ç®—æ›´å¤šæ•°æ®ç‰¹å¾
        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
        missing_summary = data.isnull().sum().to_dict()
        
        template = ChatPromptTemplate.from_messages([
            ("human", """
            ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œæ­£åœ¨å¸®åŠ©ç”¨æˆ·åˆ†æåˆšä¸Šä¼ çš„æ•°æ®é›†ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æä¾›ä¸“ä¸šçš„åˆå§‹åˆ†æå»ºè®®ï¼š
            
            ğŸ“Š æ•°æ®åŸºæœ¬ä¿¡æ¯ï¼š
            - æ•°æ®é›†å¤§å°ï¼š{rows} è¡Œ Ã— {columns} åˆ—
            - å†…å­˜ä½¿ç”¨ï¼š{memory_usage} MB
            - ç¼ºå¤±å€¼æ€»æ•°ï¼š{missing_values}
            - é‡å¤è¡Œæ•°ï¼š{duplicate_rows}
            
            ğŸ“ˆ æ•°æ®ç±»å‹åˆ†å¸ƒï¼š
            - æ•°å€¼å‹åˆ—ï¼š{numeric_cols} (å…±{num_numeric}åˆ—)
            - åˆ†ç±»å‹åˆ—ï¼š{categorical_cols} (å…±{num_categorical}åˆ—)
            
            ğŸ” ç¼ºå¤±å€¼è¯¦æƒ…ï¼š
            {missing_summary}
            
            è¯·ä»ä»¥ä¸‹è§’åº¦æä¾›ä¸“ä¸šå»ºè®®ï¼š
            
            1. **æ•°æ®è´¨é‡è¯„ä¼°** (0-100åˆ†)
               - æ•°æ®å®Œæ•´æ€§è¯„åˆ†
               - æ•°æ®ä¸€è‡´æ€§è¯„ä¼°
               - æ½œåœ¨é—®é¢˜è¯†åˆ«
            
            2. **æ•°æ®ç‰¹å¾åˆ†æ**
               - æ•°æ®é›†ç±»å‹åˆ¤æ–­ï¼ˆæ—¶é—´åºåˆ—/æ¨ªæˆªé¢/é¢æ¿æ•°æ®ç­‰ï¼‰
               - ä¸»è¦å˜é‡ç±»å‹åˆ†æ
               - æ•°æ®è§„æ¨¡è¯„ä¼°
            
            3. **æ¸…æ´—å»ºè®®**
               - ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥
               - æ•°æ®ç±»å‹è½¬æ¢å»ºè®®
               - å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•
            
            4. **åˆ†ææ–¹å‘æ¨è**
               - é€‚åˆçš„æ¢ç´¢æ€§åˆ†ææ–¹æ³•
               - å¯è§†åŒ–å»ºè®®
               - å»ºæ¨¡å¯èƒ½æ€§è¯„ä¼°
            
            5. **ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®**
               - ä¼˜å…ˆå¤„ç†çš„é—®é¢˜
               - æ¨èçš„åˆ†ææµç¨‹
               - æ³¨æ„äº‹é¡¹æé†’
            
            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå†…å®¹è¦ä¸“ä¸šã€å®ç”¨ã€æ˜“æ‡‚ã€‚æ ¼å¼è¦æ¸…æ™°ï¼Œä½¿ç”¨markdownæ ¼å¼ã€‚
            """)
        ])
        
        chain = template | self.llm
        result = chain.invoke({
            "rows": data_info['rows'],
            "columns": data_info['columns'],
            "memory_usage": data_info['memory_usage'],
            "missing_values": data_info['missing_values'],
            "duplicate_rows": data_info['duplicate_rows'],
            "numeric_cols": str(numeric_cols),
            "num_numeric": len(numeric_cols),
            "categorical_cols": str(categorical_cols),
            "num_categorical": len(categorical_cols),
            "missing_summary": str(missing_summary)
        })
        
        return result.content
    
    # ==================== æ•°æ®æ¸…æ´—é¡µé¢AIåŠŸèƒ½ ====================
    
    def suggest_cleaning_strategy(self, data: pd.DataFrame, cleaning_issue: str) -> str:
        """
        ä¸ºæ•°æ®æ¸…æ´—æä¾›æ™ºèƒ½å»ºè®®ï¼ˆæ•°æ®æ¸…æ´—é¡µé¢ï¼‰
        
        Args:
            data: æ•°æ®æ¡†
            cleaning_issue: æ¸…æ´—é—®é¢˜ç±»å‹ï¼ˆmissing_values/duplicates/outliers/data_typesï¼‰
            
        Returns:
            str: æ¸…æ´—å»ºè®®
        """
        # è·å–å…·ä½“çš„é—®é¢˜ä¿¡æ¯
        if cleaning_issue == "missing_values":
            missing_info = data.isnull().sum()
            missing_ratio = missing_info / len(data)
            problem_details = f"ç¼ºå¤±å€¼è¯¦æƒ…ï¼š\n{missing_info.to_dict()}\nç¼ºå¤±æ¯”ä¾‹ï¼š\n{missing_ratio.to_dict()}"
        elif cleaning_issue == "duplicates":
            duplicate_count = data.duplicated().sum()
            problem_details = f"é‡å¤è¡Œæ•°ï¼š{duplicate_count}ï¼Œé‡å¤æ¯”ä¾‹ï¼š{duplicate_count/len(data)*100:.2f}%"
        elif cleaning_issue == "outliers":
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            outlier_info = {}
            for col in numeric_cols:
                Q1 = data[col].quantile(0.25)
                Q3 = data[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = data[(data[col] < Q1 - 1.5 * IQR) | (data[col] > Q3 + 1.5 * IQR)]
                outlier_info[col] = len(outliers)
            problem_details = f"å¼‚å¸¸å€¼è¯¦æƒ…ï¼š{outlier_info}"
        else:
            problem_details = "æ•°æ®ç±»å‹é—®é¢˜"
        
        template = ChatPromptTemplate.from_messages([
            ("human", """
            ä½ æ˜¯ä¸€ä½æ•°æ®æ¸…æ´—ä¸“å®¶ï¼Œæ­£åœ¨å¸®åŠ©ç”¨æˆ·è§£å†³æ•°æ®è´¨é‡é—®é¢˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æä¾›ä¸“ä¸šçš„æ¸…æ´—å»ºè®®ï¼š
            
            ğŸ” æ¸…æ´—é—®é¢˜ç±»å‹ï¼š{cleaning_issue}
            
            ğŸ“Š æ•°æ®åŸºæœ¬ä¿¡æ¯ï¼š
            - æ•°æ®é›†å¤§å°ï¼š{data_size}
            - æ•°æ®ç±»å‹åˆ†å¸ƒï¼š{data_types}
            
            âš ï¸ é—®é¢˜è¯¦æƒ…ï¼š
            {problem_details}
            
            è¯·æä¾›ä»¥ä¸‹æ–¹é¢çš„ä¸“ä¸šå»ºè®®ï¼š
            
            1. **é—®é¢˜ä¸¥é‡ç¨‹åº¦è¯„ä¼°**
               - é—®é¢˜å¯¹åˆ†æçš„å½±å“ç¨‹åº¦
               - æ˜¯å¦éœ€è¦ç«‹å³å¤„ç†
               - ä¼˜å…ˆçº§æ’åº
            
            2. **æ¸…æ´—ç­–ç•¥æ¨è**
               - å…·ä½“å¤„ç†æ–¹æ³•
               - ä¸åŒæ–¹æ³•çš„ä¼˜ç¼ºç‚¹
               - æ¨èçš„å¤„ç†é¡ºåº
            
            3. **æ“ä½œæ­¥éª¤æŒ‡å¯¼**
               - è¯¦ç»†çš„æ¸…æ´—æ­¥éª¤
               - å‚æ•°è®¾ç½®å»ºè®®
               - æ³¨æ„äº‹é¡¹æé†’
            
            4. **è´¨é‡éªŒè¯æ–¹æ³•**
               - æ¸…æ´—æ•ˆæœè¯„ä¼°æŒ‡æ ‡
               - éªŒè¯æ–¹æ³•
               - æˆåŠŸæ ‡å‡†
            
            5. **æœ€ä½³å®è·µå»ºè®®**
               - è¡Œä¸šæ ‡å‡†åšæ³•
               - å¸¸è§é”™è¯¯é¿å…
               - æ•ˆç‡ä¼˜åŒ–å»ºè®®
            
            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå†…å®¹è¦å…·ä½“ã€å¯æ“ä½œã€ä¸“ä¸šã€‚ä½¿ç”¨markdownæ ¼å¼ï¼Œçªå‡ºé‡ç‚¹ã€‚
            """)
        ])
        
        chain = template | self.llm
        result = chain.invoke({
            "cleaning_issue": cleaning_issue,
            "data_size": f"{len(data)}è¡Œ Ã— {len(data.columns)}åˆ—",
            "data_types": str(data.dtypes.value_counts().to_dict()),
            "problem_details": problem_details
        })
        
        return result.content
    
    # ==================== è‡ªåŠ¨æ•°æ®åˆ†æé¡µé¢AIåŠŸèƒ½ ====================
    
    def interpret_auto_analysis(self, data: pd.DataFrame, analysis_results: Dict[str, Any]) -> str:
        """
        è§£é‡Šè‡ªåŠ¨æ•°æ®åˆ†æç»“æœï¼ˆè‡ªåŠ¨æ•°æ®åˆ†æé¡µé¢ï¼‰
        
        Args:
            data: æ•°æ®æ¡†
            analysis_results: è‡ªåŠ¨åˆ†æç»“æœ
            
        Returns:
            str: è§£é‡Šå’Œå»ºè®®
        """
        template = ChatPromptTemplate.from_messages([
            ("human", """
            ä½ æ˜¯ä¸€ä½æ•°æ®åˆ†æä¸“å®¶ï¼Œæ­£åœ¨å¸®åŠ©ç”¨æˆ·ç†è§£è‡ªåŠ¨æ•°æ®åˆ†æçš„ç»“æœã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æä¾›ä¸“ä¸šçš„è§£é‡Šå’Œå»ºè®®ï¼š
            
            ğŸ“Š æ•°æ®é›†ä¿¡æ¯ï¼š
            - æ•°æ®è§„æ¨¡ï¼š{data_size}
            - æ•°æ®ç±»å‹ï¼š{data_types}
            
            ğŸ“ˆ è‡ªåŠ¨åˆ†æç»“æœï¼š
            {analysis_results}
            
            è¯·ä»ä»¥ä¸‹è§’åº¦æä¾›ä¸“ä¸šè§£é‡Šï¼š
            
            1. **ç»“æœè§£è¯»**
               - å…³é”®å‘ç°æ€»ç»“
               - æ•°æ®æ¨¡å¼è¯†åˆ«
               - å¼‚å¸¸æƒ…å†µè¯´æ˜
            
            2. **ä¸šåŠ¡æ„ä¹‰**
               - å‘ç°çš„å®é™…æ„ä¹‰
               - æ½œåœ¨çš„ä¸šåŠ¡æ´å¯Ÿ
               - å†³ç­–æ”¯æŒå»ºè®®
            
            3. **æ·±å…¥åˆ†æå»ºè®®**
               - éœ€è¦è¿›ä¸€æ­¥æ¢ç´¢çš„æ–¹å‘
               - æ¨èçš„åˆ†ææ–¹æ³•
               - é‡ç‚¹å…³æ³¨çš„æ•°æ®ç‰¹å¾
            
            4. **å¯è§†åŒ–å»ºè®®**
               - é€‚åˆçš„å›¾è¡¨ç±»å‹
               - å±•ç¤ºé‡ç‚¹
               - è§†è§‰æ•ˆæœä¼˜åŒ–
            
            5. **ä¸‹ä¸€æ­¥è¡ŒåŠ¨**
               - ä¼˜å…ˆåˆ†ææ–¹å‘
               - å»ºæ¨¡å¯èƒ½æ€§
               - æŠ¥å‘Šé‡ç‚¹
            
            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå†…å®¹è¦ä¸“ä¸šã€æ˜“æ‡‚ã€æœ‰å®ç”¨ä»·å€¼ã€‚ä½¿ç”¨markdownæ ¼å¼ï¼Œç»“æ„æ¸…æ™°ã€‚
            """)
        ])
        
        chain = template | self.llm
        result = chain.invoke({
            "data_size": f"{len(data)}è¡Œ Ã— {len(data.columns)}åˆ—",
            "data_types": str(data.dtypes.value_counts().to_dict()),
            "analysis_results": str(analysis_results)
        })
        
        return result.content
    
    # ==================== é«˜çº§å¯è§†åŒ–é¡µé¢AIåŠŸèƒ½ ====================
    
    def suggest_visualization(self, data: pd.DataFrame, analysis_goal: str) -> str:
        """
        ä¸ºå¯è§†åŒ–æä¾›æ™ºèƒ½å»ºè®®ï¼ˆé«˜çº§å¯è§†åŒ–é¡µé¢ï¼‰
        
        Args:
            data: æ•°æ®æ¡†
            analysis_goal: åˆ†æç›®æ ‡ï¼ˆtrend_analysis/distribution_comparison/correlation_analysis/pattern_detectionï¼‰
            
        Returns:
            str: å¯è§†åŒ–å»ºè®®
        """
        # åˆ†ææ•°æ®ç‰¹å¾
        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
        
        template = ChatPromptTemplate.from_messages([
            ("human", """
            ä½ æ˜¯ä¸€ä½æ•°æ®å¯è§†åŒ–ä¸“å®¶ï¼Œæ­£åœ¨å¸®åŠ©ç”¨æˆ·é€‰æ‹©åˆé€‚çš„å¯è§†åŒ–æ–¹æ³•ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æä¾›ä¸“ä¸šçš„å¯è§†åŒ–å»ºè®®ï¼š
            
            ğŸ¯ åˆ†æç›®æ ‡ï¼š{analysis_goal}
            
            ğŸ“Š æ•°æ®ç‰¹å¾ï¼š
            - æ•°æ®è§„æ¨¡ï¼š{data_size}
            - æ•°å€¼å‹å˜é‡ï¼š{numeric_cols} (å…±{num_numeric}ä¸ª)
            - åˆ†ç±»å‹å˜é‡ï¼š{categorical_cols} (å…±{num_categorical}ä¸ª)
            
            è¯·ä»ä»¥ä¸‹è§’åº¦æä¾›ä¸“ä¸šå»ºè®®ï¼š
            
            1. **å›¾è¡¨ç±»å‹æ¨è**
               - æœ€é€‚åˆçš„å›¾è¡¨ç±»å‹
               - ä¸åŒå›¾è¡¨çš„ä¼˜ç¼ºç‚¹
               - ç»„åˆä½¿ç”¨å»ºè®®
            
            2. **å˜é‡é€‰æ‹©æŒ‡å¯¼**
               - ä¸»è¦å˜é‡æ¨è
               - è¾…åŠ©å˜é‡å»ºè®®
               - åˆ†ç»„å˜é‡é€‰æ‹©
            
            3. **å¯è§†åŒ–è®¾è®¡å»ºè®®**
               - é¢œè‰²æ­é…å»ºè®®
               - å¸ƒå±€ä¼˜åŒ–
               - äº¤äº’åŠŸèƒ½æ¨è
            
            4. **æ´å¯ŸæŒ–æ˜æŒ‡å¯¼**
               - é‡ç‚¹å…³æ³¨çš„æ•°æ®ç‰¹å¾
               - å¼‚å¸¸æ¨¡å¼è¯†åˆ«
               - è¶‹åŠ¿åˆ†æè¦ç‚¹
            
            5. **æœ€ä½³å®è·µæé†’**
               - å¯è§†åŒ–åŸåˆ™
               - å¸¸è§é”™è¯¯é¿å…
               - æ•ˆæœä¼˜åŒ–æŠ€å·§
            
            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå†…å®¹è¦å…·ä½“ã€å®ç”¨ã€ä¸“ä¸šã€‚ä½¿ç”¨markdownæ ¼å¼ï¼Œæä¾›å¯æ“ä½œçš„å»ºè®®ã€‚
            """)
        ])
        
        chain = template | self.llm
        result = chain.invoke({
            "analysis_goal": analysis_goal,
            "data_size": f"{len(data)}è¡Œ Ã— {len(data.columns)}åˆ—",
            "numeric_cols": str(numeric_cols),
            "num_numeric": len(numeric_cols),
            "categorical_cols": str(categorical_cols),
            "num_categorical": len(categorical_cols)
        })
        
        return result.content
    
    def explain_chart_insights(self, chart_type: str, data: pd.DataFrame, 
                             chart_config: Dict[str, Any], chart_stats: Dict[str, Any]) -> str:
        """
        è§£é‡Šå›¾è¡¨æ´å¯Ÿï¼ˆé«˜çº§å¯è§†åŒ–é¡µé¢ï¼‰
        
        Args:
            chart_type: å›¾è¡¨ç±»å‹
            data: æ•°æ®æ¡†
            chart_config: å›¾è¡¨é…ç½®
            chart_stats: å›¾è¡¨ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            str: æ´å¯Ÿè§£é‡Š
        """
        template = ChatPromptTemplate.from_messages([
            ("human", """
            ä½ æ˜¯ä¸€ä½æ•°æ®å¯è§†åŒ–ä¸“å®¶ï¼Œæ­£åœ¨å¸®åŠ©ç”¨æˆ·ç†è§£å›¾è¡¨ä¸­çš„æ´å¯Ÿã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æä¾›ä¸“ä¸šçš„è§£é‡Šï¼š
            
            ğŸ“Š å›¾è¡¨ä¿¡æ¯ï¼š
            - å›¾è¡¨ç±»å‹ï¼š{chart_type}
            - æ•°æ®è§„æ¨¡ï¼š{data_size}
            - å›¾è¡¨é…ç½®ï¼š{chart_config}
            
            ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯ï¼š
            {chart_stats}
            
            è¯·ä»ä»¥ä¸‹è§’åº¦æä¾›ä¸“ä¸šè§£é‡Šï¼š
            
            1. **å›¾è¡¨å«ä¹‰è§£è¯»**
               - å›¾è¡¨å±•ç¤ºçš„æ ¸å¿ƒä¿¡æ¯
               - æ•°æ®å…³ç³»è¯´æ˜
               - å…³é”®ç‰¹å¾è¯†åˆ«
            
            2. **æ•°æ®æ¨¡å¼åˆ†æ**
               - å‘ç°çš„æ¨¡å¼æˆ–è¶‹åŠ¿
               - å¼‚å¸¸æƒ…å†µè¯´æ˜
               - åˆ†å¸ƒç‰¹å¾æè¿°
            
            3. **ä¸šåŠ¡æ´å¯Ÿ**
               - å®é™…ä¸šåŠ¡æ„ä¹‰
               - å†³ç­–æ”¯æŒä¿¡æ¯
               - æ½œåœ¨æœºä¼šæˆ–é£é™©
            
            4. **è¿›ä¸€æ­¥åˆ†æå»ºè®®**
               - éœ€è¦æ·±å…¥æ¢ç´¢çš„æ–¹å‘
               - ç›¸å…³åˆ†ææ¨è
               - éªŒè¯æ–¹æ³•å»ºè®®
            
            5. **å¯è§†åŒ–ä¼˜åŒ–å»ºè®®**
               - å›¾è¡¨æ”¹è¿›æ–¹å‘
               - å±•ç¤ºæ•ˆæœä¼˜åŒ–
               - äº¤äº’åŠŸèƒ½å»ºè®®
            
            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå†…å®¹è¦ä¸“ä¸šã€æ˜“æ‡‚ã€æœ‰å®ç”¨ä»·å€¼ã€‚ä½¿ç”¨markdownæ ¼å¼ï¼Œçªå‡ºé‡ç‚¹ã€‚
            """)
        ])
        
        chain = template | self.llm
        result = chain.invoke({
            "chart_type": chart_type,
            "data_size": f"{len(data)}è¡Œ Ã— {len(data.columns)}åˆ—",
            "chart_config": str(chart_config),
            "chart_stats": str(chart_stats)
        })
        
        return result.content
    
    # ==================== ç»Ÿè®¡åˆ†æé¡µé¢AIåŠŸèƒ½ ====================
    
    def suggest_statistical_tests(self, data: pd.DataFrame, analysis_question: str) -> str:
        """
        æ¨èç»Ÿè®¡æ£€éªŒæ–¹æ³•ï¼ˆç»Ÿè®¡åˆ†æé¡µé¢ï¼‰
        
        Args:
            data: æ•°æ®æ¡†
            analysis_question: åˆ†æé—®é¢˜
            
        Returns:
            str: ç»Ÿè®¡æ£€éªŒå»ºè®®
        """
        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
        
        template = ChatPromptTemplate.from_messages([
            ("human", """
            ä½ æ˜¯ä¸€ä½ç»Ÿè®¡å­¦ä¸“å®¶ï¼Œæ­£åœ¨å¸®åŠ©ç”¨æˆ·é€‰æ‹©åˆé€‚çš„ç»Ÿè®¡æ£€éªŒæ–¹æ³•ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æä¾›ä¸“ä¸šçš„å»ºè®®ï¼š
            
            â“ åˆ†æé—®é¢˜ï¼š{analysis_question}
            
            ğŸ“Š æ•°æ®ç‰¹å¾ï¼š
            - æ•°æ®è§„æ¨¡ï¼š{data_size}
            - æ•°å€¼å‹å˜é‡ï¼š{numeric_cols} (å…±{num_numeric}ä¸ª)
            - åˆ†ç±»å‹å˜é‡ï¼š{categorical_cols} (å…±{num_categorical}ä¸ª)
            
            è¯·ä»ä»¥ä¸‹è§’åº¦æä¾›ä¸“ä¸šå»ºè®®ï¼š
            
            1. **æ£€éªŒæ–¹æ³•æ¨è**
               - æœ€é€‚åˆçš„ç»Ÿè®¡æ£€éªŒ
               - æ£€éªŒæ–¹æ³•é€‰æ‹©ç†ç”±
               - æ›¿ä»£æ–¹æ¡ˆè¯´æ˜
            
            2. **å‰ææ¡ä»¶æ£€æŸ¥**
               - æ•°æ®åˆ†å¸ƒè¦æ±‚
               - æ ·æœ¬é‡è¦æ±‚
               - ç‹¬ç«‹æ€§å‡è®¾
            
            3. **å®æ–½æ­¥éª¤æŒ‡å¯¼**
               - è¯¦ç»†çš„æ“ä½œæ­¥éª¤
               - å‚æ•°è®¾ç½®å»ºè®®
               - æ³¨æ„äº‹é¡¹æé†’
            
            4. **ç»“æœè§£é‡ŠæŒ‡å¯¼**
               - å¦‚ä½•è§£è¯»æ£€éªŒç»“æœ
               - æ˜¾è‘—æ€§æ°´å¹³è¯´æ˜
               - æ•ˆåº”é‡è®¡ç®—å»ºè®®
            
            5. **æŠ¥å‘Šæ’°å†™å»ºè®®**
               - ç»“æœæŠ¥å‘Šæ ¼å¼
               - å…³é”®ä¿¡æ¯å±•ç¤º
               - ç»“è®ºè¡¨è¿°æ–¹å¼
            
            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå†…å®¹è¦ä¸“ä¸šã€å‡†ç¡®ã€å¯æ“ä½œã€‚ä½¿ç”¨markdownæ ¼å¼ï¼Œæä¾›å…·ä½“çš„ç»Ÿè®¡å»ºè®®ã€‚
            """)
        ])
        
        chain = template | self.llm
        result = chain.invoke({
            "analysis_question": analysis_question,
            "data_size": f"{len(data)}è¡Œ Ã— {len(data.columns)}åˆ—",
            "numeric_cols": str(numeric_cols),
            "num_numeric": len(numeric_cols),
            "categorical_cols": str(categorical_cols),
            "num_categorical": len(categorical_cols)
        })
        
        return result.content
    
    def interpret_statistical_results(self, test_type: str, test_results: Dict[str, Any], 
                                    data_context: str) -> str:
        """
        è§£é‡Šç»Ÿè®¡æ£€éªŒç»“æœï¼ˆç»Ÿè®¡åˆ†æé¡µé¢ï¼‰
        
        Args:
            test_type: æ£€éªŒç±»å‹
            test_results: æ£€éªŒç»“æœ
            data_context: æ•°æ®ä¸Šä¸‹æ–‡
            
        Returns:
            str: ç»“æœè§£é‡Š
        """
        template = ChatPromptTemplate.from_messages([
            ("human", """
            ä½ æ˜¯ä¸€ä½ç»Ÿè®¡å­¦ä¸“å®¶ï¼Œæ­£åœ¨å¸®åŠ©ç”¨æˆ·ç†è§£ç»Ÿè®¡æ£€éªŒç»“æœã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æä¾›ä¸“ä¸šçš„è§£é‡Šï¼š
            
            ğŸ”¬ æ£€éªŒä¿¡æ¯ï¼š
            - æ£€éªŒç±»å‹ï¼š{test_type}
            - æ•°æ®ä¸Šä¸‹æ–‡ï¼š{data_context}
            
            ğŸ“Š æ£€éªŒç»“æœï¼š
            {test_results}
            
            è¯·ä»ä»¥ä¸‹è§’åº¦æä¾›ä¸“ä¸šè§£é‡Šï¼š
            
            1. **ç»“æœå«ä¹‰è§£è¯»**
               - ç»Ÿè®¡é‡çš„å«ä¹‰
               - på€¼çš„è§£é‡Š
               - ç½®ä¿¡åŒºé—´è¯´æ˜
            
            2. **ç»Ÿè®¡æ˜¾è‘—æ€§åˆ¤æ–­**
               - æ˜¾è‘—æ€§æ°´å¹³è¯´æ˜
               - æ‹’ç»æˆ–æ¥å—åŸå‡è®¾
               - ç»Ÿè®¡æ„ä¹‰è§£é‡Š
            
            3. **å®é™…æ„ä¹‰åˆ†æ**
               - ç»“æœçš„ä¸šåŠ¡å«ä¹‰
               - å®é™…å½±å“ç¨‹åº¦
               - å†³ç­–æ”¯æŒä¿¡æ¯
            
            4. **å±€é™æ€§è¯´æ˜**
               - æ£€éªŒçš„å‡è®¾æ¡ä»¶
               - å¯èƒ½çš„åå·®æ¥æº
               - ç»“æœé€‚ç”¨èŒƒå›´
            
            5. **è¿›ä¸€æ­¥åˆ†æå»ºè®®**
               - è¡¥å……æ£€éªŒæ¨è
               - æ·±å…¥åˆ†ææ–¹å‘
               - éªŒè¯æ–¹æ³•å»ºè®®
            
            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå†…å®¹è¦ä¸“ä¸šã€å‡†ç¡®ã€æ˜“æ‡‚ã€‚ä½¿ç”¨markdownæ ¼å¼ï¼Œç¡®ä¿è§£é‡Šæ¸…æ™°ã€‚
            """)
        ])
        
        chain = template | self.llm
        result = chain.invoke({
            "test_type": test_type,
            "data_context": data_context,
            "test_results": str(test_results)
        })
        
        return result.content
    
    # ==================== æœºå™¨å­¦ä¹ é¡µé¢AIåŠŸèƒ½ ====================
    
    def suggest_ml_approach(self, data: pd.DataFrame, task_type: str, 
                           target_col: str, feature_cols: List[str]) -> str:
        """
        æ¨èæœºå™¨å­¦ä¹ æ–¹æ³•ï¼ˆæœºå™¨å­¦ä¹ é¡µé¢ï¼‰
        
        Args:
            data: æ•°æ®æ¡†
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆåˆ†ç±»/å›å½’/èšç±»ï¼‰
            target_col: ç›®æ ‡å˜é‡
            feature_cols: ç‰¹å¾å˜é‡
            
        Returns:
            str: æœºå™¨å­¦ä¹ å»ºè®®
        """
        # è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        numeric_stats = data[feature_cols].describe() if feature_cols else {}
        target_distribution = data[target_col].value_counts() if task_type == "åˆ†ç±»" else data[target_col].describe()
        
        template = ChatPromptTemplate.from_messages([
            ("human", """
            ä½ æ˜¯ä¸€ä½æœºå™¨å­¦ä¹ ä¸“å®¶ï¼Œæ­£åœ¨å¸®åŠ©ç”¨æˆ·é€‰æ‹©åˆé€‚çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æä¾›ä¸“ä¸šçš„å»ºè®®ï¼š
            
            ğŸ¤– ä»»åŠ¡ä¿¡æ¯ï¼š
            - ä»»åŠ¡ç±»å‹ï¼š{task_type}
            - ç›®æ ‡å˜é‡ï¼š{target_col}
            - ç‰¹å¾å˜é‡ï¼š{feature_cols} (å…±{num_features}ä¸ª)
            
            ğŸ“Š æ•°æ®ç‰¹å¾ï¼š
            - æ•°æ®è§„æ¨¡ï¼š{data_size}
            - ç‰¹å¾ç»Ÿè®¡ï¼š{numeric_stats}
            - ç›®æ ‡åˆ†å¸ƒï¼š{target_distribution}
            
            è¯·ä»ä»¥ä¸‹è§’åº¦æä¾›ä¸“ä¸šå»ºè®®ï¼š
            
            1. **ç®—æ³•æ¨è**
               - æœ€é€‚åˆçš„ç®—æ³•
               - ç®—æ³•é€‰æ‹©ç†ç”±
               - å¤‡é€‰æ–¹æ¡ˆè¯´æ˜
            
            2. **æ•°æ®é¢„å¤„ç†å»ºè®®**
               - ç‰¹å¾å·¥ç¨‹æ–¹æ³•
               - æ•°æ®æ¸…æ´—ç­–ç•¥
               - ç‰¹å¾é€‰æ‹©å»ºè®®
            
            3. **æ¨¡å‹è®­ç»ƒæŒ‡å¯¼**
               - è®­ç»ƒå‚æ•°è®¾ç½®
               - éªŒè¯æ–¹æ³•é€‰æ‹©
               - è¶…å‚æ•°è°ƒä¼˜ç­–ç•¥
            
            4. **è¯„ä¼°æ–¹æ³•æ¨è**
               - è¯„ä¼°æŒ‡æ ‡é€‰æ‹©
               - äº¤å‰éªŒè¯è®¾ç½®
               - æ¨¡å‹æ¯”è¾ƒæ–¹æ³•
            
            5. **å®æ–½æ­¥éª¤è§„åˆ’**
               - è¯¦ç»†çš„æ“ä½œæµç¨‹
               - å…³é”®èŠ‚ç‚¹æ£€æŸ¥
               - é£é™©æ§åˆ¶å»ºè®®
            
            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå†…å®¹è¦ä¸“ä¸šã€å…·ä½“ã€å¯æ“ä½œã€‚ä½¿ç”¨markdownæ ¼å¼ï¼Œæä¾›å®ç”¨çš„å»ºè®®ã€‚
            """)
        ])
        
        chain = template | self.llm
        result = chain.invoke({
            "task_type": task_type,
            "target_col": target_col,
            "feature_cols": str(feature_cols),
            "num_features": len(feature_cols),
            "data_size": f"{len(data)}è¡Œ Ã— {len(data.columns)}åˆ—",
            "numeric_stats": str(numeric_stats),
            "target_distribution": str(target_distribution)
        })
        
        return result.content
    
    def interpret_ml_results(self, task_type: str, model_results: Dict[str, Any], 
                           feature_importance: Optional[Dict[str, float]] = None) -> str:
        """
        è§£é‡Šæœºå™¨å­¦ä¹ ç»“æœï¼ˆæœºå™¨å­¦ä¹ é¡µé¢ï¼‰
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹
            model_results: æ¨¡å‹ç»“æœ
            feature_importance: ç‰¹å¾é‡è¦æ€§
            
        Returns:
            str: ç»“æœè§£é‡Š
        """
        template = ChatPromptTemplate.from_messages([
            ("human", """
            ä½ æ˜¯ä¸€ä½æœºå™¨å­¦ä¹ ä¸“å®¶ï¼Œæ­£åœ¨å¸®åŠ©ç”¨æˆ·ç†è§£æœºå™¨å­¦ä¹ æ¨¡å‹çš„ç»“æœã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æä¾›ä¸“ä¸šçš„è§£é‡Šï¼š
            
            ğŸ¤– æ¨¡å‹ä¿¡æ¯ï¼š
            - ä»»åŠ¡ç±»å‹ï¼š{task_type}
            - æ¨¡å‹ç»“æœï¼š{model_results}
            - ç‰¹å¾é‡è¦æ€§ï¼š{feature_importance}
            
            è¯·ä»ä»¥ä¸‹è§’åº¦æä¾›ä¸“ä¸šè§£é‡Šï¼š
            
            1. **æ¨¡å‹æ€§èƒ½è¯„ä¼°**
               - æ€§èƒ½æŒ‡æ ‡è§£è¯»
               - æ¨¡å‹è¡¨ç°è¯„ä»·
               - ä¸åŸºå‡†æ¯”è¾ƒ
            
            2. **ç»“æœå¯é æ€§åˆ†æ**
               - è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆæ£€æŸ¥
               - æ³›åŒ–èƒ½åŠ›è¯„ä¼°
               - ç¨³å®šæ€§åˆ†æ
            
            3. **ç‰¹å¾é‡è¦æ€§è§£é‡Š**
               - å…³é”®ç‰¹å¾è¯†åˆ«
               - ç‰¹å¾å½±å“ç¨‹åº¦
               - ä¸šåŠ¡æ„ä¹‰åˆ†æ
            
            4. **æ¨¡å‹å±€é™æ€§**
               - å‡è®¾æ¡ä»¶è¯´æ˜
               - é€‚ç”¨èŒƒå›´é™åˆ¶
               - æ½œåœ¨åå·®åˆ†æ
            
            5. **åº”ç”¨å»ºè®®**
               - å®é™…åº”ç”¨æŒ‡å¯¼
               - éƒ¨ç½²æ³¨æ„äº‹é¡¹
               - æŒç»­ä¼˜åŒ–å»ºè®®
            
            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå†…å®¹è¦ä¸“ä¸šã€å‡†ç¡®ã€å®ç”¨ã€‚ä½¿ç”¨markdownæ ¼å¼ï¼Œç¡®ä¿è§£é‡Šæ¸…æ™°æ˜“æ‡‚ã€‚
            """)
        ])
        
        chain = template | self.llm
        result = chain.invoke({
            "task_type": task_type,
            "model_results": str(model_results),
            "feature_importance": str(feature_importance) if feature_importance else "æ— ç‰¹å¾é‡è¦æ€§ä¿¡æ¯"
        })
        
        return result.content
    
    # ==================== æŠ¥å‘Šç”Ÿæˆé¡µé¢AIåŠŸèƒ½ ====================
    
    def suggest_report_structure(self, data: pd.DataFrame, analysis_summary: Dict[str, Any]) -> str:
        """
        å»ºè®®æŠ¥å‘Šç»“æ„ï¼ˆæŠ¥å‘Šç”Ÿæˆé¡µé¢ï¼‰
        
        Args:
            data: æ•°æ®æ¡†
            analysis_summary: åˆ†ææ€»ç»“
            
        Returns:
            str: æŠ¥å‘Šç»“æ„å»ºè®®
        """
        template = ChatPromptTemplate.from_messages([
            ("human", """
            ä½ æ˜¯ä¸€ä½æ•°æ®åˆ†ææŠ¥å‘Šä¸“å®¶ï¼Œæ­£åœ¨å¸®åŠ©ç”¨æˆ·è®¾è®¡ä¸“ä¸šçš„åˆ†ææŠ¥å‘Šã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æä¾›æŠ¥å‘Šç»“æ„å»ºè®®ï¼š
            
            ğŸ“Š æ•°æ®ä¿¡æ¯ï¼š
            - æ•°æ®è§„æ¨¡ï¼š{data_size}
            - æ•°æ®ç±»å‹ï¼š{data_types}
            
            ğŸ“ˆ åˆ†ææ€»ç»“ï¼š
            {analysis_summary}
            
            è¯·ä»ä»¥ä¸‹è§’åº¦æä¾›ä¸“ä¸šå»ºè®®ï¼š
            
            1. **æŠ¥å‘Šç»“æ„è®¾è®¡**
               - ç« èŠ‚ç»„ç»‡å»ºè®®
               - å†…å®¹é€»è¾‘é¡ºåº
               - é‡ç‚¹çªå‡ºç­–ç•¥
            
            2. **å…³é”®å†…å®¹è§„åˆ’**
               - æ ¸å¿ƒå‘ç°å±•ç¤º
               - æ•°æ®æ”¯æ’‘è¦æ±‚
               - ç»“è®ºå»ºè®®æ¡†æ¶
            
            3. **å¯è§†åŒ–è®¾è®¡**
               - å›¾è¡¨ç±»å‹é€‰æ‹©
               - å±•ç¤ºé¡ºåºå®‰æ’
               - è§†è§‰æ•ˆæœä¼˜åŒ–
            
            4. **å—ä¼—é€‚é…å»ºè®®**
               - æŠ€æœ¯æ·±åº¦è°ƒæ•´
               - è¯­è¨€é£æ ¼å»ºè®®
               - é‡ç‚¹å†…å®¹çªå‡º
            
            5. **æŠ¥å‘Šè´¨é‡æå‡**
               - ä¸“ä¸šæœ¯è¯­ä½¿ç”¨
               - é€»è¾‘æ¸…æ™°åº¦
               - å¯è¯»æ€§ä¼˜åŒ–
            
            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå†…å®¹è¦ä¸“ä¸šã€å…¨é¢ã€å®ç”¨ã€‚ä½¿ç”¨markdownæ ¼å¼ï¼Œæä¾›å…·ä½“çš„æŠ¥å‘Šå»ºè®®ã€‚
            """)
        ])
        
        chain = template | self.llm
        result = chain.invoke({
            "data_size": f"{len(data)}è¡Œ Ã— {len(data.columns)}åˆ—",
            "data_types": str(data.dtypes.value_counts().to_dict()),
            "analysis_summary": str(analysis_summary)
        })
        
        return result.content
    
    # ==================== é€šç”¨æ™ºèƒ½é—®ç­”åŠŸèƒ½ ====================
    
    def answer_data_question(self, question: str, data_context: str, 
                           current_page: str = "é€šç”¨") -> str:
        """
        å›ç­”æ•°æ®ç›¸å…³é—®é¢˜ï¼ˆé€šç”¨åŠŸèƒ½ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            data_context: æ•°æ®ä¸Šä¸‹æ–‡
            current_page: å½“å‰é¡µé¢
            
        Returns:
            str: å›ç­”
        """
        template = ChatPromptTemplate.from_messages([
            ("human", """
            ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œæ­£åœ¨{current_page}é¡µé¢å¸®åŠ©ç”¨æˆ·è§£ç­”é—®é¢˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æä¾›ä¸“ä¸šå›ç­”ï¼š
            
            â“ ç”¨æˆ·é—®é¢˜ï¼š{question}
            
            ğŸ“Š æ•°æ®ä¸Šä¸‹æ–‡ï¼š{data_context}
            
            è¯·æä¾›ä»¥ä¸‹æ–¹é¢çš„ä¸“ä¸šå›ç­”ï¼š
            
            1. **ç›´æ¥å›ç­”**
               - é’ˆå¯¹é—®é¢˜çš„å…·ä½“ç­”æ¡ˆ
               - æ ¸å¿ƒè¦ç‚¹æ€»ç»“
               - å…³é”®ä¿¡æ¯æå–
            
            2. **è¯¦ç»†è§£é‡Š**
               - ç›¸å…³æ¦‚å¿µè¯´æ˜
               - æ–¹æ³•åŸç†ä»‹ç»
               - èƒŒæ™¯çŸ¥è¯†è¡¥å……
            
            3. **æ“ä½œæŒ‡å¯¼**
               - å…·ä½“æ“ä½œæ­¥éª¤
               - å·¥å…·ä½¿ç”¨å»ºè®®
               - å‚æ•°è®¾ç½®æŒ‡å¯¼
            
            4. **æ³¨æ„äº‹é¡¹**
               - å¸¸è§é”™è¯¯æé†’
               - é£é™©ç‚¹è¯´æ˜
               - æœ€ä½³å®è·µå»ºè®®
            
            5. **æ‰©å±•å»ºè®®**
               - ç›¸å…³åˆ†ææ–¹å‘
               - æ·±å…¥å­¦ä¹ èµ„æº
               - è¿›é˜¶æ–¹æ³•æ¨è
            
            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå†…å®¹è¦å‡†ç¡®ã€ä¸“ä¸šã€æ˜“æ‡‚ã€‚ä½¿ç”¨markdownæ ¼å¼ï¼Œç¡®ä¿å›ç­”ç»“æ„æ¸…æ™°ã€‚
            """)
        ])
        
        chain = template | self.llm
        result = chain.invoke({
            "current_page": current_page,
            "question": question,
            "data_context": data_context
        })
        
        return result.content


# å…¨å±€AIåŠ©æ‰‹å®ä¾‹
ai_assistant = None

def get_ai_assistant() -> Optional[DataAnalysisAI]:
    """
    è·å–AIåŠ©æ‰‹å®ä¾‹
    
    Returns:
        DataAnalysisAIå®ä¾‹æˆ–None
    """
    global ai_assistant
    if ai_assistant is None:
        try:
            ai_assistant = DataAnalysisAI()
        except ValueError as e:
            print(f"AIåŠ©æ‰‹åˆ›å»ºå¤±è´¥ - é…ç½®é”™è¯¯: {e}")
            return None
        except Exception as e:
            print(f"AIåŠ©æ‰‹åˆ›å»ºå¤±è´¥ - å…¶ä»–é”™è¯¯: {e}")
            return None
    return ai_assistant
