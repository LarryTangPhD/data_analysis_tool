"""
æ–°æ‰‹æ¨¡å¼AIåŠ©æ‰‹æ¨¡å—
ä¸“é—¨ä¸ºæ–°æ‰‹æ¨¡å¼æä¾›æ•™è‚²å¯¼å‘çš„AIæ™ºèƒ½åŠ©æ‰‹åŠŸèƒ½
"""

from langchain.prompts import ChatPromptTemplate
from langchain_openai.chat_models.base import BaseChatOpenAI
import os
import pandas as pd
import numpy as np
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings('ignore')

class BeginnerModeAI:
    """æ–°æ‰‹æ¨¡å¼AIåŠ©æ‰‹ç±»"""
    
    def __init__(self, api_key: Optional[str] = None, base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1"):
        """
        åˆå§‹åŒ–æ–°æ‰‹æ¨¡å¼AIåŠ©æ‰‹
        
        Args:
            api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è·å–
            base_url: APIåŸºç¡€URL
        """
        self.api_key = api_key or os.getenv("DASHSCOPE_API_KEY")
        if not self.api_key:
            raise ValueError("DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®æˆ–æœªæä¾›")
        
        self.base_url = base_url
        self.llm = self._get_llm()
        
        # æ–°æ‰‹æ¨¡å¼é¢„è®¾é—®é¢˜æ¨¡æ¿
        self.preset_questions = {
            "welcome": [
                "ä»€ä¹ˆæ˜¯æ•°æ®åˆ†æï¼Ÿ",
                "æˆ‘éœ€è¦å­¦ä¹ å“ªäº›åŸºç¡€çŸ¥è¯†ï¼Ÿ",
                "æ•´ä¸ªå­¦ä¹ æµç¨‹å¤§æ¦‚éœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ",
                "æˆ‘åº”è¯¥ä»å“ªé‡Œå¼€å§‹ï¼Ÿ",
                "æ•°æ®åˆ†æåœ¨å“ªäº›é¢†åŸŸåº”ç”¨ï¼Ÿ",
                "å­¦ä¹ æ•°æ®åˆ†ææœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ"
            ],
            "data_upload": [
                "ä»€ä¹ˆæ˜¯CSVæ–‡ä»¶ï¼Ÿ",
                "å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ•°æ®æ–‡ä»¶ï¼Ÿ",
                "æ•°æ®ä¸Šä¼ å¤±è´¥æ€ä¹ˆåŠï¼Ÿ",
                "ä¸ºä»€ä¹ˆéœ€è¦äº†è§£æ•°æ®ç»“æ„ï¼Ÿ",
                "Excelæ–‡ä»¶å’ŒCSVæ–‡ä»¶æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
                "æ•°æ®æ–‡ä»¶å¤ªå¤§æ€ä¹ˆåŠï¼Ÿ"
            ],
            "data_structure": [
                "ä»€ä¹ˆæ˜¯æ•°å€¼å‹æ•°æ®ï¼Ÿ",
                "ä»€ä¹ˆæ˜¯åˆ†ç±»å‹æ•°æ®ï¼Ÿ",
                "ç¼ºå¤±å€¼æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ",
                "ä¸ºä»€ä¹ˆè¦æ£€æŸ¥æ•°æ®ç±»å‹ï¼Ÿ",
                "å¦‚ä½•è¯†åˆ«æ•°æ®ç±»å‹ï¼Ÿ",
                "æ•°æ®ç±»å‹é”™è¯¯ä¼šæœ‰ä»€ä¹ˆå½±å“ï¼Ÿ"
            ],
            "data_cleaning": [
                "ä»€ä¹ˆæ˜¯æ•°æ®æ¸…æ´—ï¼Ÿ",
                "ä¸ºä»€ä¹ˆè¦å¤„ç†ç¼ºå¤±å€¼ï¼Ÿ",
                "é‡å¤æ•°æ®æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ",
                "å¼‚å¸¸å€¼æ˜¯ä»€ä¹ˆï¼Ÿ",
                "å¦‚ä½•å¤„ç†ç¼ºå¤±å€¼ï¼Ÿ",
                "æ•°æ®æ¸…æ´—çš„æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ",
                "æ¸…æ´—åçš„æ•°æ®è´¨é‡å¦‚ä½•è¯„ä¼°ï¼Ÿ"
            ],
            "visualization": [
                "ä»€ä¹ˆæ—¶å€™ç”¨æŸ±çŠ¶å›¾ï¼Ÿ",
                "æ•£ç‚¹å›¾èƒ½çœ‹å‡ºä»€ä¹ˆï¼Ÿ",
                "å¦‚ä½•é€‰æ‹©åˆé€‚çš„å›¾è¡¨ï¼Ÿ",
                "ä»€ä¹ˆæ˜¯æ•°æ®åˆ†å¸ƒï¼Ÿ",
                "ç®±çº¿å›¾èƒ½å‘Šè¯‰æˆ‘ä»¬ä»€ä¹ˆï¼Ÿ",
                "çƒ­åŠ›å›¾æœ‰ä»€ä¹ˆç”¨ï¼Ÿ",
                "å¦‚ä½•è§£è¯»å›¾è¡¨ç»“æœï¼Ÿ"
            ],
            "statistical_analysis": [
                "ä»€ä¹ˆæ˜¯æè¿°æ€§ç»Ÿè®¡ï¼Ÿ",
                "ç›¸å…³ç³»æ•°æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ",
                "ä¸ºä»€ä¹ˆè¦åšç»Ÿè®¡åˆ†æï¼Ÿ",
                "å¦‚ä½•è§£é‡Šç»Ÿè®¡ç»“æœï¼Ÿ",
                "å‡å€¼å’Œä¸­ä½æ•°æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
                "æ ‡å‡†å·®ä»£è¡¨ä»€ä¹ˆï¼Ÿ",
                "å¦‚ä½•åˆ¤æ–­ç›¸å…³æ€§æ˜¯å¦æ˜¾è‘—ï¼Ÿ"
            ],
            "report": [
                "å¦‚ä½•å†™æ•°æ®åˆ†ææŠ¥å‘Šï¼Ÿ",
                "æŠ¥å‘Šåº”è¯¥åŒ…å«å“ªäº›å†…å®¹ï¼Ÿ",
                "å¦‚ä½•å±•ç¤ºåˆ†æç»“æœï¼Ÿ",
                "æœ‰ä»€ä¹ˆæ³¨æ„äº‹é¡¹ï¼Ÿ",
                "å¦‚ä½•è®©æŠ¥å‘Šæ›´æœ‰è¯´æœåŠ›ï¼Ÿ",
                "æŠ¥å‘Šçš„ç»“æ„åº”è¯¥å¦‚ä½•å®‰æ’ï¼Ÿ",
                "å¦‚ä½•æ€»ç»“åˆ†æç»“è®ºï¼Ÿ"
            ]
        }
    
    def _get_llm(self, temperature: float = 0.7) -> BaseChatOpenAI:
        """åˆ›å»ºLLMå®ä¾‹"""
        return BaseChatOpenAI(
            model="qwen-plus",
            openai_api_key=self.api_key,
            openai_api_base=self.base_url,
            temperature=temperature,
            max_tokens=2000,
            timeout=60,
            request_timeout=60
        )
    
    # ==================== æ–°æ‰‹æ¨¡å¼ä¸“ç”¨AIåŠŸèƒ½ ====================
    
    def answer_beginner_question(self, question: str, current_step: str, data_context: str = "") -> str:
        """
        å›ç­”æ–°æ‰‹é—®é¢˜ï¼ˆæ•™è‚²å¯¼å‘ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            current_step: å½“å‰å­¦ä¹ æ­¥éª¤
            data_context: æ•°æ®ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            str: æ•™è‚²å¯¼å‘çš„å›ç­”
        """
        template = ChatPromptTemplate.from_messages([
            ("human", """
            ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®åˆ†æå¯¼å¸ˆï¼Œæ­£åœ¨æŒ‡å¯¼ä¸€ä½åˆå­¦è€…å­¦ä¹ æ•°æ®åˆ†æã€‚
            
            ğŸ“š å½“å‰å­¦ä¹ é˜¶æ®µï¼š{current_step}
            â“ å­¦ç”Ÿé—®é¢˜ï¼š{question}
            ğŸ“Š æ•°æ®ä¸Šä¸‹æ–‡ï¼š{data_context}
            
            è¯·ä»¥å¯¼å¸ˆçš„èº«ä»½å›ç­”ï¼Œè¦æ±‚ï¼š
            
            1. **é€šä¿—æ˜“æ‡‚**ï¼šç”¨ç®€å•æ˜äº†çš„è¯­è¨€è§£é‡Šæ¦‚å¿µ
            2. **å¾ªåºæ¸è¿›**ï¼šä»åŸºç¡€æ¦‚å¿µå¼€å§‹ï¼Œé€æ­¥æ·±å…¥
            3. **å®ä¾‹è¯´æ˜**ï¼šç”¨å…·ä½“ä¾‹å­å¸®åŠ©ç†è§£
            4. **é¼“åŠ±å­¦ä¹ **ï¼šç»™äºˆç§¯æçš„å­¦ä¹ å»ºè®®
            5. **è”ç³»å®é™…**ï¼šè¯´æ˜åœ¨å®é™…å·¥ä½œä¸­çš„åº”ç”¨
            
            å›ç­”ç»“æ„ï¼š
            - ğŸ¯ ç›´æ¥å›ç­”
            - ğŸ“– æ¦‚å¿µè§£é‡Š
            - ğŸ’¡ å®ä¾‹è¯´æ˜
            - ğŸš€ å­¦ä¹ å»ºè®®
            - âš ï¸ æ³¨æ„äº‹é¡¹
            
            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­æ°”å‹å¥½ã€é¼“åŠ±æ€§å¼ºã€‚
            """)
        ])
        
        chain = template | self.llm
        result = chain.invoke({
            "current_step": current_step,
            "question": question,
            "data_context": data_context or "æš‚æ— æ•°æ®"
        })
        
        return result.content
    
    def provide_learning_guidance(self, current_step: str, user_progress: Dict[str, Any]) -> str:
        """
        æä¾›å­¦ä¹ æŒ‡å¯¼
        
        Args:
            current_step: å½“å‰å­¦ä¹ æ­¥éª¤
            user_progress: ç”¨æˆ·å­¦ä¹ è¿›åº¦
            
        Returns:
            str: å­¦ä¹ æŒ‡å¯¼
        """
        template = ChatPromptTemplate.from_messages([
            ("human", """
            ä½ æ˜¯ä¸€ä½æ•°æ®åˆ†æå­¦ä¹ å¯¼å¸ˆï¼Œæ­£åœ¨ä¸ºä¸€ä½åˆå­¦è€…æä¾›å­¦ä¹ æŒ‡å¯¼ã€‚
            
            ğŸ“š å½“å‰å­¦ä¹ é˜¶æ®µï¼š{current_step}
            ğŸ“Š å­¦ä¹ è¿›åº¦ï¼š{user_progress}
            
            è¯·æä¾›ï¼š
            
            1. **å­¦ä¹ é‡ç‚¹**ï¼šå½“å‰é˜¶æ®µéœ€è¦æŒæ¡çš„æ ¸å¿ƒæ¦‚å¿µ
            2. **å­¦ä¹ æ–¹æ³•**ï¼šæ¨èçš„å­¦ä¹ æ–¹æ³•å’ŒæŠ€å·§
            3. **å¸¸è§è¯¯åŒº**ï¼šæé†’å¯èƒ½é‡åˆ°çš„å¸¸è§é—®é¢˜
            4. **ä¸‹ä¸€æ­¥å»ºè®®**ï¼šä¸ºä¸‹ä¸€é˜¶æ®µå­¦ä¹ åšå‡†å¤‡
            5. **é¼“åŠ±è¯è¯­**ï¼šç»™äºˆå­¦ä¹ åŠ¨åŠ›å’Œä¿¡å¿ƒ
            
            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­æ°”æ¸©æš–ã€é¼“åŠ±æ€§å¼ºã€‚
            """)
        ])
        
        chain = template | self.llm
        result = chain.invoke({
            "current_step": current_step,
            "user_progress": str(user_progress)
        })
        
        return result.content
    
    def explain_concept(self, concept: str, current_step: str) -> str:
        """
        è§£é‡Šæ¦‚å¿µï¼ˆæ–°æ‰‹å‹å¥½ï¼‰
        
        Args:
            concept: è¦è§£é‡Šçš„æ¦‚å¿µ
            current_step: å½“å‰å­¦ä¹ æ­¥éª¤
            
        Returns:
            str: æ¦‚å¿µè§£é‡Š
        """
        template = ChatPromptTemplate.from_messages([
            ("human", """
            ä½ æ˜¯ä¸€ä½æ•°æ®åˆ†æå¯¼å¸ˆï¼Œæ­£åœ¨å‘åˆå­¦è€…è§£é‡Šæ¦‚å¿µã€‚
            
            ğŸ“– è¦è§£é‡Šçš„æ¦‚å¿µï¼š{concept}
            ğŸ“š å½“å‰å­¦ä¹ é˜¶æ®µï¼š{current_step}
            
            è¯·æä¾›ï¼š
            
            1. **ç®€å•å®šä¹‰**ï¼šç”¨æœ€é€šä¿—çš„è¯­è¨€å®šä¹‰æ¦‚å¿µ
            2. **ç”Ÿæ´»ç±»æ¯”**ï¼šç”¨ç”Ÿæ´»ä¸­çš„ä¾‹å­ç±»æ¯”
            3. **é‡è¦æ€§è¯´æ˜**ï¼šä¸ºä»€ä¹ˆéœ€è¦äº†è§£è¿™ä¸ªæ¦‚å¿µ
            4. **å®é™…åº”ç”¨**ï¼šåœ¨æ•°æ®åˆ†æä¸­å¦‚ä½•ä½¿ç”¨
            5. **å­¦ä¹ å»ºè®®**ï¼šå¦‚ä½•æ›´å¥½åœ°ç†è§£è¿™ä¸ªæ¦‚å¿µ
            
            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€ç®€å•æ˜“æ‡‚ï¼Œé¿å…ä¸“ä¸šæœ¯è¯­ã€‚
            """)
        ])
        
        chain = template | self.llm
        result = chain.invoke({
            "concept": concept,
            "current_step": current_step
        })
        
        return result.content
    
    def analyze_learning_progress(self, user_actions: List[str], current_step: str) -> str:
        """
        åˆ†æå­¦ä¹ è¿›åº¦
        
        Args:
            user_actions: ç”¨æˆ·æ“ä½œè®°å½•
            current_step: å½“å‰å­¦ä¹ æ­¥éª¤
            
        Returns:
            str: å­¦ä¹ è¿›åº¦åˆ†æ
        """
        template = ChatPromptTemplate.from_messages([
            ("human", """
            ä½ æ˜¯ä¸€ä½æ•°æ®åˆ†æå­¦ä¹ å¯¼å¸ˆï¼Œæ­£åœ¨åˆ†æå­¦ç”Ÿçš„å­¦ä¹ è¿›åº¦ã€‚
            
            ğŸ“Š ç”¨æˆ·æ“ä½œè®°å½•ï¼š{user_actions}
            ğŸ“š å½“å‰å­¦ä¹ é˜¶æ®µï¼š{current_step}
            
            è¯·åˆ†æï¼š
            
            1. **å­¦ä¹ è¡¨ç°**ï¼šè¯„ä¼°å½“å‰å­¦ä¹ æ•ˆæœ
            2. **å­¦ä¹ ä¹ æƒ¯**ï¼šåˆ†æå­¦ä¹ æ–¹æ³•å’Œä¹ æƒ¯
            3. **è–„å¼±ç¯èŠ‚**ï¼šè¯†åˆ«éœ€è¦åŠ å¼ºçš„åœ°æ–¹
            4. **å­¦ä¹ å»ºè®®**ï¼šæä¾›æ”¹è¿›å»ºè®®
            5. **é¼“åŠ±è¯è¯­**ï¼šç»™äºˆç§¯æåé¦ˆ
            
            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­æ°”ç§¯æã€å»ºè®¾æ€§å¼ºã€‚
            """)
        ])
        
        chain = template | self.llm
        result = chain.invoke({
            "user_actions": str(user_actions),
            "current_step": current_step
        })
        
        return result.content
    
    def get_preset_questions(self, step: str) -> List[str]:
        """è·å–é¢„è®¾é—®é¢˜"""
        return self.preset_questions.get(step, [])
    
    def suggest_next_steps(self, current_step: str, user_performance: Dict[str, Any]) -> str:
        """
        å»ºè®®ä¸‹ä¸€æ­¥å­¦ä¹ 
        
        Args:
            current_step: å½“å‰å­¦ä¹ æ­¥éª¤
            user_performance: ç”¨æˆ·è¡¨ç°
            
        Returns:
            str: ä¸‹ä¸€æ­¥å»ºè®®
        """
        template = ChatPromptTemplate.from_messages([
            ("human", """
            ä½ æ˜¯ä¸€ä½æ•°æ®åˆ†æå­¦ä¹ å¯¼å¸ˆï¼Œæ­£åœ¨ä¸ºå­¦ç”Ÿè§„åˆ’ä¸‹ä¸€æ­¥å­¦ä¹ ã€‚
            
            ğŸ“š å½“å‰å­¦ä¹ é˜¶æ®µï¼š{current_step}
            ğŸ“Š å­¦ä¹ è¡¨ç°ï¼š{user_performance}
            
            è¯·æä¾›ï¼š
            
            1. **å­¦ä¹ å»ºè®®**ï¼šä¸‹ä¸€æ­¥åº”è¯¥å­¦ä¹ ä»€ä¹ˆ
            2. **å­¦ä¹ æ–¹æ³•**ï¼šæ¨èçš„å­¦ä¹ æ–¹æ³•
            3. **æ—¶é—´å®‰æ’**ï¼šå»ºè®®çš„å­¦ä¹ æ—¶é—´
            4. **é‡ç‚¹æé†’**ï¼šéœ€è¦æ³¨æ„çš„é‡ç‚¹
            5. **å­¦ä¹ ç›®æ ‡**ï¼šè®¾å®šæ˜ç¡®çš„å­¦ä¹ ç›®æ ‡
            
            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå»ºè®®å…·ä½“ã€å¯æ“ä½œã€‚
            """)
        ])
        
        chain = template | self.llm
        result = chain.invoke({
            "current_step": current_step,
            "user_performance": str(user_performance)
        })
        
        return result.content

# å…¨å±€æ–°æ‰‹æ¨¡å¼AIåŠ©æ‰‹å®ä¾‹
beginner_ai_assistant = None

def get_beginner_ai_assistant() -> BeginnerModeAI:
    """è·å–æ–°æ‰‹æ¨¡å¼AIåŠ©æ‰‹å®ä¾‹"""
    global beginner_ai_assistant
    if beginner_ai_assistant is None:
        try:
            beginner_ai_assistant = BeginnerModeAI()
        except Exception as e:
            print(f"AIåŠ©æ‰‹åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
            return None
    return beginner_ai_assistant
