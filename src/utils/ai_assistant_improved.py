"""
æ”¹è¿›çš„AIåŠ©æ‰‹æ¨¡å—
æ·»åŠ æ›´å¥½çš„é”™è¯¯å¤„ç†å’Œè°ƒè¯•ä¿¡æ¯
"""

from langchain.prompts import ChatPromptTemplate
from langchain_openai.chat_models.base import BaseChatOpenAI
import os
import pandas as pd
import numpy as np
from typing import Optional, Dict, Any, List
import warnings
import logging
warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataAnalysisAI:
    """æ•°æ®åˆ†æAIåŠ©æ‰‹ç±»"""
    
    def __init__(self, api_key: Optional[str] = None, base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1"):
        """
        åˆå§‹åŒ–AIåŠ©æ‰‹
        
        Args:
            api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è·å–
            base_url: APIåŸºç¡€URL
        """
        logger.info("å¼€å§‹åˆå§‹åŒ–AIåŠ©æ‰‹...")
        
        # è·å–APIå¯†é’¥
        self.api_key = api_key or os.getenv("DASHSCOPE_API_KEY")
        logger.info(f"APIå¯†é’¥çŠ¶æ€: {'å·²è®¾ç½®' if self.api_key else 'æœªè®¾ç½®'}")
        
        if not self.api_key:
            logger.error("DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®æˆ–æœªæä¾›")
            raise ValueError("DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®æˆ–æœªæä¾›")
        
        self.base_url = base_url
        logger.info(f"ä½¿ç”¨APIåŸºç¡€URL: {self.base_url}")
        
        # åˆ›å»ºLLMå®ä¾‹
        try:
            self.llm = self._get_llm()
            logger.info("LLMå®ä¾‹åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            logger.error(f"LLMå®ä¾‹åˆ›å»ºå¤±è´¥: {e}")
            raise
    
    def _get_llm(self, temperature: float = 0.7) -> BaseChatOpenAI:
        """
        åˆ›å»ºLLMå®ä¾‹
        
        Args:
            temperature: åˆ›é€ æ€§å‚æ•°
            
        Returns:
            BaseChatOpenAIå®ä¾‹
        """
        logger.info("æ­£åœ¨åˆ›å»ºLLMå®ä¾‹...")
        
        try:
            llm = BaseChatOpenAI(
                model="qwen-plus",
                openai_api_key=self.api_key,
                openai_api_base=self.base_url,
                temperature=temperature,
                max_tokens=2000,
                timeout=60,
                request_timeout=60
            )
            logger.info("LLMå®ä¾‹åˆ›å»ºæˆåŠŸ")
            return llm
        except Exception as e:
            logger.error(f"LLMå®ä¾‹åˆ›å»ºå¤±è´¥: {e}")
            raise
    
    def test_connection(self) -> bool:
        """
        æµ‹è¯•APIè¿æ¥
        
        Returns:
            bool: è¿æ¥æ˜¯å¦æˆåŠŸ
        """
        logger.info("æµ‹è¯•APIè¿æ¥...")
        
        try:
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•æç¤º
            test_template = ChatPromptTemplate.from_messages([
                ("human", "è¯·å›ç­”ï¼šä½ å¥½")
            ])
            
            chain = test_template | self.llm
            result = chain.invoke({})
            
            logger.info("APIè¿æ¥æµ‹è¯•æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"APIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def analyze_uploaded_data(self, data: pd.DataFrame, data_info: Dict[str, Any]) -> str:
        """
        åˆ†æä¸Šä¼ çš„æ•°æ®å¹¶æä¾›åˆå§‹å»ºè®®
        
        Args:
            data: æ•°æ®æ¡†
            data_info: æ•°æ®åŸºæœ¬ä¿¡æ¯
            
        Returns:
            str: åˆ†æå»ºè®®
        """
        logger.info("å¼€å§‹åˆ†æä¸Šä¼ çš„æ•°æ®...")
        
        try:
            # è®¡ç®—æ›´å¤šæ•°æ®ç‰¹å¾
            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
            categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
            missing_summary = data.isnull().sum().to_dict()
            
            logger.info(f"æ•°æ®ç‰¹å¾: æ•°å€¼å‹åˆ—{len(numeric_cols)}ä¸ª, åˆ†ç±»å‹åˆ—{len(categorical_cols)}ä¸ª")
            
            template = ChatPromptTemplate.from_messages([
                ("human", """
                ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œæ­£åœ¨å¸®åŠ©ç”¨æˆ·åˆ†æåˆšä¸Šä¼ çš„æ•°æ®é›†ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æä¾›ä¸“ä¸šçš„åˆå§‹åˆ†æå»ºè®®ï¼š
                
                ğŸ“Š æ•°æ®åŸºæœ¬ä¿¡æ¯ï¼š
                - æ•°æ®é›†å¤§å°ï¼š{rows} è¡Œ Ã— {columns} åˆ—
                - å†…å­˜ä½¿ç”¨ï¼š{memory_usage} MB
                - ç¼ºå¤±å€¼æ€»æ•°ï¼š{missing_values}
                - é‡å¤è¡Œæ•°ï¼š{duplicate_rows}
                
                ğŸ“ˆ æ•°æ®ç±»å‹åˆ†å¸ƒï¼š
                - æ•°å€¼å‹åˆ—ï¼š{numeric_cols} (å…±{num_numeric}åˆ—)
                - åˆ†ç±»å‹åˆ—ï¼š{categorical_cols} (å…±{num_categorical}åˆ—)
                
                ğŸ” ç¼ºå¤±å€¼è¯¦æƒ…ï¼š
                {missing_summary}
                
                è¯·ä»ä»¥ä¸‹è§’åº¦æä¾›ä¸“ä¸šå»ºè®®ï¼š
                
                1. **æ•°æ®è´¨é‡è¯„ä¼°** (0-100åˆ†)
                   - æ•°æ®å®Œæ•´æ€§è¯„åˆ†
                   - æ•°æ®ä¸€è‡´æ€§è¯„ä¼°
                   - æ½œåœ¨é—®é¢˜è¯†åˆ«
                
                2. **æ•°æ®ç‰¹å¾åˆ†æ**
                   - æ•°æ®é›†ç±»å‹åˆ¤æ–­ï¼ˆæ—¶é—´åºåˆ—/æ¨ªæˆªé¢/é¢æ¿æ•°æ®ç­‰ï¼‰
                   - ä¸»è¦å˜é‡ç±»å‹åˆ†æ
                   - æ•°æ®è§„æ¨¡è¯„ä¼°
                
                3. **æ¸…æ´—å»ºè®®**
                   - ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥
                   - æ•°æ®ç±»å‹è½¬æ¢å»ºè®®
                   - å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•
                
                4. **åˆ†ææ–¹å‘æ¨è**
                   - é€‚åˆçš„æ¢ç´¢æ€§åˆ†ææ–¹æ³•
                   - å¯è§†åŒ–å»ºè®®
                   - å»ºæ¨¡å¯èƒ½æ€§è¯„ä¼°
                
                5. **ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®**
                   - ä¼˜å…ˆå¤„ç†çš„é—®é¢˜
                   - æ¨èçš„åˆ†ææµç¨‹
                   - æ³¨æ„äº‹é¡¹æé†’
                
                è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå†…å®¹è¦ä¸“ä¸šã€å®ç”¨ã€æ˜“æ‡‚ã€‚æ ¼å¼è¦æ¸…æ™°ï¼Œä½¿ç”¨markdownæ ¼å¼ã€‚
                """)
            ])
            
            chain = template | self.llm
            result = chain.invoke({
                "rows": data_info['rows'],
                "columns": data_info['columns'],
                "memory_usage": data_info['memory_usage'],
                "missing_values": data_info['missing_values'],
                "duplicate_rows": data_info['duplicate_rows'],
                "numeric_cols": str(numeric_cols),
                "num_numeric": len(numeric_cols),
                "categorical_cols": str(categorical_cols),
                "num_categorical": len(categorical_cols),
                "missing_summary": str(missing_summary)
            })
            
            logger.info("æ•°æ®åˆ†æå®Œæˆ")
            return result.content
            
        except Exception as e:
            logger.error(f"æ•°æ®åˆ†æå¤±è´¥: {e}")
            raise
    
    def answer_data_question(self, question: str, data_context: str, 
                           current_page: str = "é€šç”¨") -> str:
        """
        å›ç­”æ•°æ®ç›¸å…³é—®é¢˜
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            data_context: æ•°æ®ä¸Šä¸‹æ–‡
            current_page: å½“å‰é¡µé¢
            
        Returns:
            str: å›ç­”
        """
        logger.info(f"å›ç­”ç”¨æˆ·é—®é¢˜: {question[:50]}...")
        
        try:
            template = ChatPromptTemplate.from_messages([
                ("human", """
                ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œæ­£åœ¨{current_page}é¡µé¢å¸®åŠ©ç”¨æˆ·è§£ç­”é—®é¢˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æä¾›ä¸“ä¸šå›ç­”ï¼š
                
                â“ ç”¨æˆ·é—®é¢˜ï¼š{question}
                
                ğŸ“Š æ•°æ®ä¸Šä¸‹æ–‡ï¼š{data_context}
                
                è¯·æä¾›ä»¥ä¸‹æ–¹é¢çš„ä¸“ä¸šå›ç­”ï¼š
                
                1. **ç›´æ¥å›ç­”**
                   - é’ˆå¯¹é—®é¢˜çš„å…·ä½“ç­”æ¡ˆ
                   - æ ¸å¿ƒè¦ç‚¹æ€»ç»“
                   - å…³é”®ä¿¡æ¯æå–
                
                2. **è¯¦ç»†è§£é‡Š**
                   - ç›¸å…³æ¦‚å¿µè¯´æ˜
                   - æ–¹æ³•åŸç†ä»‹ç»
                   - èƒŒæ™¯çŸ¥è¯†è¡¥å……
                
                3. **æ“ä½œæŒ‡å¯¼**
                   - å…·ä½“æ“ä½œæ­¥éª¤
                   - å·¥å…·ä½¿ç”¨å»ºè®®
                   - å‚æ•°è®¾ç½®æŒ‡å¯¼
                
                4. **æ³¨æ„äº‹é¡¹**
                   - å¸¸è§é”™è¯¯æé†’
                   - é£é™©ç‚¹è¯´æ˜
                   - æœ€ä½³å®è·µå»ºè®®
                
                5. **æ‰©å±•å»ºè®®**
                   - ç›¸å…³åˆ†ææ–¹å‘
                   - æ·±å…¥å­¦ä¹ èµ„æº
                   - è¿›é˜¶æ–¹æ³•æ¨è
                
                è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå†…å®¹è¦å‡†ç¡®ã€ä¸“ä¸šã€æ˜“æ‡‚ã€‚ä½¿ç”¨markdownæ ¼å¼ï¼Œç¡®ä¿å›ç­”ç»“æ„æ¸…æ™°ã€‚
                """)
            ])
            
            chain = template | self.llm
            result = chain.invoke({
                "current_page": current_page,
                "question": question,
                "data_context": data_context
            })
            
            logger.info("é—®é¢˜å›ç­”å®Œæˆ")
            return result.content
            
        except Exception as e:
            logger.error(f"é—®é¢˜å›ç­”å¤±è´¥: {e}")
            raise

# å…¨å±€AIåŠ©æ‰‹å®ä¾‹
ai_assistant = None

def get_ai_assistant() -> Optional[DataAnalysisAI]:
    """
    è·å–AIåŠ©æ‰‹å®ä¾‹
    
    Returns:
        DataAnalysisAIå®ä¾‹æˆ–None
    """
    global ai_assistant
    
    logger.info("è·å–AIåŠ©æ‰‹å®ä¾‹...")
    
    if ai_assistant is None:
        try:
            logger.info("åˆ›å»ºæ–°çš„AIåŠ©æ‰‹å®ä¾‹...")
            ai_assistant = DataAnalysisAI()
            logger.info("AIåŠ©æ‰‹å®ä¾‹åˆ›å»ºæˆåŠŸ")
        except ValueError as e:
            logger.error(f"AIåŠ©æ‰‹åˆ›å»ºå¤±è´¥ - é…ç½®é”™è¯¯: {e}")
            return None
        except Exception as e:
            logger.error(f"AIåŠ©æ‰‹åˆ›å»ºå¤±è´¥ - å…¶ä»–é”™è¯¯: {e}")
            return None
    
    return ai_assistant

def test_ai_assistant_connection() -> Dict[str, Any]:
    """
    æµ‹è¯•AIåŠ©æ‰‹è¿æ¥
    
    Returns:
        Dict: æµ‹è¯•ç»“æœ
    """
    logger.info("å¼€å§‹æµ‹è¯•AIåŠ©æ‰‹è¿æ¥...")
    
    result = {
        "success": False,
        "error": None,
        "details": {}
    }
    
    try:
        # è·å–AIåŠ©æ‰‹å®ä¾‹
        ai_assistant = get_ai_assistant()
        if ai_assistant is None:
            result["error"] = "AIåŠ©æ‰‹å®ä¾‹åˆ›å»ºå¤±è´¥"
            return result
        
        # æµ‹è¯•è¿æ¥
        if ai_assistant.test_connection():
            result["success"] = True
            result["details"]["message"] = "AIåŠ©æ‰‹è¿æ¥æ­£å¸¸"
        else:
            result["error"] = "APIè¿æ¥æµ‹è¯•å¤±è´¥"
            
    except Exception as e:
        result["error"] = str(e)
        logger.error(f"AIåŠ©æ‰‹è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
    
    return result
