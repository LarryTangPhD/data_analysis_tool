"""
æ€§èƒ½ä¼˜åŒ–æ¨¡å—
æä¾›æ•°æ®ç¼“å­˜ã€æ‡’åŠ è½½ã€å†…å­˜ä¼˜åŒ–ç­‰åŠŸèƒ½
å‚è€ƒè¡Œä¸šæœ€ä½³å®è·µä¼˜åŒ–ç”¨æˆ·ä½“éªŒ
"""

import streamlit as st
import pandas as pd
import numpy as np
from typing import Dict, Any, Optional, Callable
import time
import functools
import gc


class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–å™¨"""
        self.setup_cache_config()
        self.setup_session_state()
    
    def setup_cache_config(self):
        """è®¾ç½®ç¼“å­˜é…ç½®"""
        self.cache_config = {
            'max_entries': 100,
            'ttl': 3600,  # 1å°æ—¶
            'show_spinner': True
        }
    
    def setup_session_state(self):
        """è®¾ç½®ä¼šè¯çŠ¶æ€"""
        if 'performance_metrics' not in st.session_state:
            st.session_state.performance_metrics = {
                'load_times': {},
                'memory_usage': {},
                'cache_hits': 0,
                'cache_misses': 0
            }
        
        if 'optimization_settings' not in st.session_state:
            st.session_state.optimization_settings = {
                'enable_cache': True,
                'enable_lazy_loading': True,
                'enable_memory_optimization': True,
                'max_data_size': 1000000  # 100ä¸‡è¡Œ
            }
    
    @st.cache_data(ttl=3600, max_entries=50)
    def cached_data_processing(_data: pd.DataFrame, operation: str, **kwargs) -> Any:
        """
        ç¼“å­˜æ•°æ®å¤„ç†ç»“æœ
        
        Args:
            _data: æ•°æ®æ¡†
            operation: æ“ä½œç±»å‹
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            å¤„ç†ç»“æœ
        """
        start_time = time.time()
        
        if operation == "describe":
            result = _data.describe()
        elif operation == "correlation":
            numeric_data = _data.select_dtypes(include=[np.number])
            result = numeric_data.corr() if len(numeric_data.columns) > 1 else pd.DataFrame()
        elif operation == "missing_summary":
            result = _data.isnull().sum().to_frame('missing_count')
            result['missing_ratio'] = result['missing_count'] / len(_data)
        elif operation == "dtype_summary":
            result = _data.dtypes.value_counts().to_frame('count')
        else:
            result = None
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        if 'performance_metrics' in st.session_state:
            st.session_state.performance_metrics['load_times'][operation] = time.time() - start_time
            st.session_state.performance_metrics['cache_hits'] += 1
        
        return result
    
    def optimize_dataframe(_df: pd.DataFrame) -> pd.DataFrame:
        """
        ä¼˜åŒ–DataFrameå†…å­˜ä½¿ç”¨
        
        Args:
            _df: åŸå§‹DataFrame
            
        Returns:
            ä¼˜åŒ–åçš„DataFrame
        """
        start_memory = _df.memory_usage(deep=True).sum()
        
        # ä¼˜åŒ–æ•°å€¼åˆ—
        for col in _df.select_dtypes(include=['int64']).columns:
            col_min = _df[col].min()
            col_max = _df[col].max()
            
            if col_min >= np.iinfo(np.int8).min and col_max <= np.iinfo(np.int8).max:
                _df[col] = _df[col].astype(np.int8)
            elif col_min >= np.iinfo(np.int16).min and col_max <= np.iinfo(np.int16).max:
                _df[col] = _df[col].astype(np.int16)
            elif col_min >= np.iinfo(np.int32).min and col_max <= np.iinfo(np.int32).max:
                _df[col] = _df[col].astype(np.int32)
        
        # ä¼˜åŒ–æµ®ç‚¹åˆ—
        for col in _df.select_dtypes(include=['float64']).columns:
            _df[col] = pd.to_numeric(_df[col], downcast='float')
        
        # ä¼˜åŒ–å­—ç¬¦ä¸²åˆ—
        for col in _df.select_dtypes(include=['object']).columns:
            if _df[col].nunique() / len(_df) < 0.5:  # å¦‚æœå”¯ä¸€å€¼æ¯”ä¾‹å°äº50%
                _df[col] = _df[col].astype('category')
        
        end_memory = _df.memory_usage(deep=True).sum()
        memory_reduction = (start_memory - end_memory) / start_memory * 100
        
        # è®°å½•å†…å­˜ä¼˜åŒ–æŒ‡æ ‡
        if 'performance_metrics' in st.session_state:
            st.session_state.performance_metrics['memory_usage']['optimization'] = {
                'before': start_memory,
                'after': end_memory,
                'reduction_percent': memory_reduction
            }
        
        return _df
    
    def lazy_load_data(data: pd.DataFrame, chunk_size: int = 10000) -> pd.DataFrame:
        """
        æ‡’åŠ è½½æ•°æ®ï¼Œåˆ†æ‰¹å¤„ç†å¤§æ•°æ®é›†
        
        Args:
            data: å®Œæ•´æ•°æ®é›†
            chunk_size: åˆ†å—å¤§å°
            
        Returns:
            å¤„ç†åçš„æ•°æ®
        """
        if len(data) <= chunk_size:
            return data
        
        # æ˜¾ç¤ºè¿›åº¦æ¡
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        processed_chunks = []
        total_chunks = (len(data) + chunk_size - 1) // chunk_size
        
        for i in range(0, len(data), chunk_size):
            chunk = data.iloc[i:i+chunk_size]
            processed_chunk = PerformanceOptimizer.optimize_dataframe(chunk)
            processed_chunks.append(processed_chunk)
            
            # æ›´æ–°è¿›åº¦
            progress = (i + chunk_size) / len(data)
            progress_bar.progress(min(progress, 1.0))
            status_text.text(f"å¤„ç†æ•°æ®ä¸­... {i+chunk_size}/{len(data)} è¡Œ")
        
        progress_bar.empty()
        status_text.empty()
        
        return pd.concat(processed_chunks, ignore_index=True)
    
    def render_optimization_controls(self):
        """æ¸²æŸ“æ€§èƒ½ä¼˜åŒ–æ§åˆ¶é¢æ¿"""
        st.sidebar.markdown("### âš¡ æ€§èƒ½è®¾ç½®")
        
        # ç¼“å­˜è®¾ç½®
        enable_cache = st.sidebar.checkbox(
            "å¯ç”¨ç¼“å­˜",
            value=st.session_state.optimization_settings['enable_cache'],
            help="ç¼“å­˜è®¡ç®—ç»“æœä»¥æé«˜æ€§èƒ½"
        )
        st.session_state.optimization_settings['enable_cache'] = enable_cache
        
        # æ‡’åŠ è½½è®¾ç½®
        enable_lazy_loading = st.sidebar.checkbox(
            "å¯ç”¨æ‡’åŠ è½½",
            value=st.session_state.optimization_settings['enable_lazy_loading'],
            help="åˆ†æ‰¹å¤„ç†å¤§æ•°æ®é›†"
        )
        st.session_state.optimization_settings['enable_lazy_loading'] = enable_lazy_loading
        
        # å†…å­˜ä¼˜åŒ–è®¾ç½®
        enable_memory_optimization = st.sidebar.checkbox(
            "å¯ç”¨å†…å­˜ä¼˜åŒ–",
            value=st.session_state.optimization_settings['enable_memory_optimization'],
            help="ä¼˜åŒ–æ•°æ®ç±»å‹ä»¥å‡å°‘å†…å­˜ä½¿ç”¨"
        )
        st.session_state.optimization_settings['enable_memory_optimization'] = enable_memory_optimization
        
        # æ€§èƒ½æŒ‡æ ‡
        if st.sidebar.button("ğŸ“Š æŸ¥çœ‹æ€§èƒ½æŒ‡æ ‡"):
            self.render_performance_metrics()
    
    def render_performance_metrics(self):
        """æ¸²æŸ“æ€§èƒ½æŒ‡æ ‡"""
        metrics = st.session_state.performance_metrics
        
        st.markdown("### ğŸ“Š æ€§èƒ½æŒ‡æ ‡")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ç¼“å­˜å‘½ä¸­", metrics.get('cache_hits', 0))
        
        with col2:
            st.metric("ç¼“å­˜æœªå‘½ä¸­", metrics.get('cache_misses', 0))
        
        with col3:
            if metrics.get('load_times'):
                avg_load_time = np.mean(list(metrics['load_times'].values()))
                st.metric("å¹³å‡åŠ è½½æ—¶é—´", f"{avg_load_time:.3f}s")
        
        with col4:
            if metrics.get('memory_usage', {}).get('optimization'):
                reduction = metrics['memory_usage']['optimization']['reduction_percent']
                st.metric("å†…å­˜ä¼˜åŒ–", f"{reduction:.1f}%")
        
        # è¯¦ç»†æŒ‡æ ‡
        if metrics.get('load_times'):
            st.markdown("**åŠ è½½æ—¶é—´è¯¦æƒ…ï¼š**")
            for operation, load_time in metrics['load_times'].items():
                st.text(f"â€¢ {operation}: {load_time:.3f}s")
        
        if metrics.get('memory_usage', {}).get('optimization'):
            opt = metrics['memory_usage']['optimization']
            st.markdown("**å†…å­˜ä¼˜åŒ–è¯¦æƒ…ï¼š**")
            st.text(f"â€¢ ä¼˜åŒ–å‰: {opt['before'] / 1024 / 1024:.2f} MB")
            st.text(f"â€¢ ä¼˜åŒ–å: {opt['after'] / 1024 / 1024:.2f} MB")
            st.text(f"â€¢ å‡å°‘: {opt['reduction_percent']:.1f}%")
    
    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        st.cache_data.clear()
        st.cache_resource.clear()
        st.success("âœ… ç¼“å­˜å·²æ¸…é™¤")
    
    def optimize_for_large_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        é’ˆå¯¹å¤§æ•°æ®é›†çš„ä¼˜åŒ–
        
        Args:
            data: åŸå§‹æ•°æ®
            
        Returns:
            ä¼˜åŒ–åçš„æ•°æ®
        """
        settings = st.session_state.optimization_settings
        
        if len(data) > settings['max_data_size']:
            st.warning(f"âš ï¸ æ•°æ®é›†è¾ƒå¤§ ({len(data):,} è¡Œ)ï¼Œå»ºè®®å¯ç”¨ä¼˜åŒ–é€‰é¡¹")
        
        if settings['enable_memory_optimization']:
            data = self.optimize_dataframe(data)
        
        if settings['enable_lazy_loading'] and len(data) > 50000:
            data = self.lazy_load_data(data)
        
        return data
    
    def monitor_performance(func: Callable) -> Callable:
        """
        æ€§èƒ½ç›‘æ§è£…é¥°å™¨
        
        Args:
            func: è¦ç›‘æ§çš„å‡½æ•°
            
        Returns:
            åŒ…è£…åçš„å‡½æ•°
        """
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            start_memory = gc.get_count()
            
            try:
                result = func(*args, **kwargs)
                
                # è®°å½•æ€§èƒ½æŒ‡æ ‡
                execution_time = time.time() - start_time
                end_memory = gc.get_count()
                memory_used = sum(end_memory) - sum(start_memory)
                
                if 'performance_metrics' in st.session_state:
                    st.session_state.performance_metrics['load_times'][func.__name__] = execution_time
                    st.session_state.performance_metrics['memory_usage'][func.__name__] = memory_used
                
                return result
                
            except Exception as e:
                st.error(f"å‡½æ•° {func.__name__} æ‰§è¡Œå¤±è´¥: {str(e)}")
                raise
        
        return wrapper
    
    def get_data_sample(self, data: pd.DataFrame, sample_size: int = 1000) -> pd.DataFrame:
        """
        è·å–æ•°æ®æ ·æœ¬ç”¨äºå¿«é€Ÿé¢„è§ˆ
        
        Args:
            data: å®Œæ•´æ•°æ®é›†
            sample_size: æ ·æœ¬å¤§å°
            
        Returns:
            æ•°æ®æ ·æœ¬
        """
        if len(data) <= sample_size:
            return data
        
        # åˆ†å±‚é‡‡æ ·ä»¥ä¿æŒæ•°æ®åˆ†å¸ƒ
        if len(data.columns) > 0:
            # é€‰æ‹©æ•°å€¼åˆ—è¿›è¡Œåˆ†å±‚
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                # åŸºäºç¬¬ä¸€ä¸ªæ•°å€¼åˆ—è¿›è¡Œåˆ†å±‚é‡‡æ ·
                sample_data = data.groupby(pd.qcut(data[numeric_cols[0]], 10, duplicates='drop')).apply(
                    lambda x: x.sample(min(len(x), sample_size // 10))
                ).reset_index(drop=True)
                return sample_data
        
        # ç®€å•éšæœºé‡‡æ ·
        return data.sample(n=sample_size, random_state=42)
    
    def render_data_size_warning(self, data: pd.DataFrame):
        """æ¸²æŸ“æ•°æ®å¤§å°è­¦å‘Š"""
        if len(data) > 100000:
            st.warning("""
            âš ï¸ **å¤§æ•°æ®é›†è­¦å‘Š**
            
            å½“å‰æ•°æ®é›†è¾ƒå¤§ï¼Œå¯èƒ½å½±å“æ€§èƒ½ã€‚å»ºè®®ï¼š
            - å¯ç”¨å†…å­˜ä¼˜åŒ–
            - ä½¿ç”¨æ•°æ®é‡‡æ ·è¿›è¡Œå¿«é€Ÿé¢„è§ˆ
            - è€ƒè™‘æ•°æ®é¢„å¤„ç†
            """)
            
            if st.button("ğŸ”§ åº”ç”¨ä¼˜åŒ–è®¾ç½®"):
                st.session_state.optimization_settings.update({
                    'enable_cache': True,
                    'enable_lazy_loading': True,
                    'enable_memory_optimization': True
                })
                st.success("âœ… ä¼˜åŒ–è®¾ç½®å·²åº”ç”¨")
    
    def cleanup_memory(self):
        """æ¸…ç†å†…å­˜"""
        gc.collect()
        st.success("âœ… å†…å­˜æ¸…ç†å®Œæˆ")


# å…¨å±€æ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹
performance_optimizer = PerformanceOptimizer()

def get_performance_optimizer():
    """è·å–æ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹"""
    return performance_optimizer
